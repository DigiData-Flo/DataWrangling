{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Effort\n",
    "\n",
    "## Intro\n",
    "\n",
    "Udacity provided 2 files for this project. A Twitter archive file with tweet information for direct download and a prediction file for programmatic download.\n",
    "The original tweet data should be downloaded via the Twitter API.\n",
    "\n",
    "The whole project deals with the following points\n",
    "* Data Gathering\n",
    "* Data Assessment\n",
    "* Data Cleaning\n",
    "* Data Insights\n",
    "\n",
    "## Data Gathering\n",
    "\n",
    "### Twitter Archiv\n",
    "The Twitter archive is a file provided by Udacity for direct download. I just had to download it and load it into a Panda data frame.\n",
    "### Prediction Datei\n",
    "I had to download this file programmatically via a url request. The only peculiarity of this file was that it used a tabulator as a separator.\n",
    "### Twitter API Download\n",
    "The download of the original tweet data with the Twitter API gave me some difficulties.\n",
    "First the procurement of\n",
    "* consumer key\n",
    "* consumer secret\n",
    "* access token\n",
    "* acess secret\n",
    "after successfully creating a developer account. I only found this data when I created a new project on the developer side.\n",
    "\n",
    "At least the tweet ID, as well as the retweet count and the favorite count and whatever else interested me should be loaded from the downloaded data into a data frame.\n",
    "\n",
    "At this point it was not clear to me that a complete json object could be loaded into a data frame. So I'm experimenting with the raw json data, which might interest me.\n",
    "\n",
    "In a project of another Udacity student I learned that you can also load entire json objects into a data frame. This then resulted in a much better overview. Thank you very much David.\n",
    "\n",
    "## Data Assessment\n",
    "I carried out the data assessment programmatically because the amount of data was too large to find anything meaningful visually.\n",
    "\n",
    "### Twitter Archiv\n",
    "* String None instead nan for many columns\n",
    "* Suspicious ratings for Denominators and Numerators\n",
    "* Not satisfactory dog names with 'a'.\n",
    "* Data types of columns timestamp and tweet_id.\n",
    "* Columns with indicators for not relevant rows, as retweet_status and source.\n",
    "* Not needed or redundant columns to remove like expandend_urls, source, in_reply.., retweeted_status...\n",
    "* The columns doggo, pupper, puppo, floofer are from one variable dog_type and belongs in one column. This is a tidiness issue.\n",
    "\n",
    "### Prediction Data\n",
    "* 66 duplicate rows based on columns jpeg_url\n",
    "* Data type of column tweet_id\n",
    "* There are 324 rows where all three predictions identified not a dog. This rows should be removed from the data set.\n",
    "* The column img_num makes no sense. Remove it.\n",
    "* The columns 'p1', 'p1_dog', 'p1_conf','p2', 'p2_dog', 'p2_conf','p3', 'p3_dog', 'p3_conf', should be grouped in two columns 'breed' and 'confidence'. This is a tidiness issue.\n",
    "\n",
    "### API Data\n",
    "\n",
    "* column name 'id' refers to the tweet_id and should be renamed to 'tweet_id' that it is harmonized with the other data sets.\n",
    "* The language columns is not interesting. The few entries wich are not english contains some mysterious text. In general this column can be dropped.\n",
    "* From retweet_status I can see that there are 163 retweets, which are accordingly to project specifications are not to consider.\n",
    "* The followers_count and the favourites_count are to extract from user column.\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "The cleaning process was very complex, even if almost everything was implemented programmatically. The ratings in particular caused difficulties because they included very strange ratings.\n",
    "\n",
    "> Meet OliviÃ©r. He takes killer selfies. Has a dog of his own. It leaps at random &amp; can't bark for shit. 10/10 &amp; 5/10 >https://t.co/6NgsQJuSBJ \n",
    "\n",
    "I suspected that it might have been a faulty extraction.\n",
    "I decided to find all 10/10 using a regular expression. The chosen expression is: `r'\\d+/\\d+'`.\n",
    "For the entries where I found more than one, I looked at the texts and decided for each text which entry is the rating.\n",
    "\n",
    "\n",
    "When I summarized the doggo, puppo, pupper, floofer columns, I noticed that some tweets named two dog types. I've chosen to accept this as a legal variable value.\n",
    "\n",
    "## Data Merging\n",
    "\n",
    "The last step after cleaning the data, was to merge all three data sets into one data frame. This is done because all data sets belongs to one analysis. I decided to do an inner join because I am only interested in tweets that are really in all data records.\n",
    "\n",
    "\n",
    "![image](./images/master_df_info.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
